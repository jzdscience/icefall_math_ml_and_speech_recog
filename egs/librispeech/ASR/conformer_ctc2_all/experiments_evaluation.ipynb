{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### jz3702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analysis on Model Performance (Word Error Rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the epoch number with best validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_20231210_gelu_clean\n",
      "Best Validation Loss Epoch:  17\n",
      "LastEpoch:  30\n",
      "run_20231206_relu_clean\n",
      "Best Validation Loss Epoch:  28\n",
      "LastEpoch:  30\n",
      "run_20231211_selu_clean\n",
      "Best Validation Loss Epoch:  6\n",
      "LastEpoch:  30\n",
      "run_20231208_singleswish_clean\n",
      "Best Validation Loss Epoch:  6\n",
      "LastEpoch:  30\n",
      "run_20231209_tanh_clean\n",
      "Best Validation Loss Epoch:  6\n",
      "LastEpoch:  30\n",
      "run_20231208_leakyRelu_clean\n",
      "Best Validation Loss Epoch:  23\n",
      "LastEpoch:  30\n",
      "run_20231210_elu_clean\n",
      "Best Validation Loss Epoch:  6\n",
      "LastEpoch:  30\n",
      "run_20231213_softplus_all_activation_clean\n",
      "Best Validation Loss Epoch:  5\n",
      "LastEpoch:  5\n",
      "run_20231211_softplus_clean\n",
      "Best Validation Loss Epoch:  26\n",
      "LastEpoch:  30\n",
      "run_20231206_doubleswish_clean\n",
      "Best Validation Loss Epoch:  23\n",
      "LastEpoch:  30\n"
     ]
    }
   ],
   "source": [
    "directory='./exp/'\n",
    "\n",
    "model_epoch_info = {}\n",
    "\n",
    "for subdir in os.listdir(directory):\n",
    "    model_checkpoint_epoch = 1\n",
    "    for file in os.listdir(directory + subdir):\n",
    "        if re.match(r'epoch.*\\.pt$', file):\n",
    "            if int(file[6:-3]) > model_checkpoint_epoch:\n",
    "                model_checkpoint = file\n",
    "                model_checkpoint_epoch = int(file[6:-3])\n",
    "\n",
    "    print(subdir)\n",
    "    model = torch.load(os.path.join(directory, subdir, model_checkpoint), map_location=torch.device('cpu'))\n",
    "    # print(model)\n",
    "    print('Best Validation Loss Epoch: ', model['best_valid_epoch'])\n",
    "    print('LastEpoch: ', model_checkpoint_epoch)\n",
    "\n",
    "    model_epoch_info.update({subdir: {'best_valid_epoch': model['best_valid_epoch'], 'last_epoch': model_checkpoint_epoch}})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k2-version': '1.24.4',\n",
       " 'k2-build-type': 'Release',\n",
       " 'k2-with-cuda': True,\n",
       " 'k2-git-sha1': '59aef5e3a52a43990f78ad33f4839f0a1680358a',\n",
       " 'k2-git-date': 'Thu Nov 23 09:08:57 2023',\n",
       " 'lhotse-version': '1.17.0.dev+git.b869488.clean',\n",
       " 'torch-version': '2.1.0+cu121',\n",
       " 'torch-cuda-available': True,\n",
       " 'torch-cuda-version': '12.1',\n",
       " 'python-version': '3.1',\n",
       " 'icefall-git-branch': 'master',\n",
       " 'icefall-git-sha1': '7c682ec-dirty',\n",
       " 'icefall-git-date': 'Sat Dec 9 06:50:50 2023',\n",
       " 'icefall-path': '/home/ipsoct4/project/icefall_w4995_mathml',\n",
       " 'k2-path': '/home/ipsoct4/anaconda3/envs/kaldi/lib/python3.11/site-packages/k2/__init__.py',\n",
       " 'lhotse-path': '/home/ipsoct4/anaconda3/envs/kaldi/lib/python3.11/site-packages/lhotse/__init__.py',\n",
       " 'hostname': 'instance-4t4',\n",
       " 'IP address': '10.182.0.2'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['env_info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Run decoders and conduct evaluation\n",
    "\n",
    "Run all experiements `run_all_decoders.sh` or \n",
    "individual experiment\n",
    "`python decode.py --epoch 30 --avg 1 --max-duration 150  --exp-dir \"$PWD/$dir_name\"  --lang-dir ../data/lang_bpe_500  --method ctc-decoding`\n",
    "\n",
    "Note you need to run individual experiement and change epoch argument manually to the best validation loss epoch obtained from step 1.1, `run_all_decoders.sh` does not support a variable epoch argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load experiments and obtain the WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='./exp/'\n",
    "\n",
    "wer_results = {}\n",
    "\n",
    "for subdir in os.listdir(directory):\n",
    "    wer_results.update({subdir: []})\n",
    "    for file in os.listdir(directory + subdir):\n",
    "        if re.match(r'wer.*clean\\.txt$', file):\n",
    "            with open(os.path.join(directory, subdir, file), 'r') as f:\n",
    "                clean_wer_text = f.readlines()\n",
    "                # extract number from string\n",
    "                clean_wer = float(clean_wer_text[1].split('\\t')[-1])\n",
    "        elif re.match(r'wer.*other\\.txt$', file):\n",
    "            with open(os.path.join(directory, subdir, file), 'r') as f:\n",
    "                other_wer_text = f.readlines()\n",
    "                # extract number from string\n",
    "                other_wer = float(other_wer_text[1].split('\\t')[-1])\n",
    "    wer_results.update({subdir: {'clean_wer': clean_wer, 'other_wer': other_wer}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Error Rate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_20231210_gelu_clean': {'clean_wer': 9.93, 'other_wer': 25.42},\n",
       " 'run_20231206_relu_clean': {'clean_wer': 12.72, 'other_wer': 33.96},\n",
       " 'run_20231211_selu_clean': {'clean_wer': 97.73, 'other_wer': 97.74},\n",
       " 'run_20231208_singleswish_clean': {'clean_wer': 98.7, 'other_wer': 98.76},\n",
       " 'run_20231209_tanh_clean': {'clean_wer': 98.92, 'other_wer': 98.8},\n",
       " 'run_20231208_leakyRelu_clean': {'clean_wer': 21.62, 'other_wer': 47.37},\n",
       " 'run_20231210_elu_clean': {'clean_wer': 99.99, 'other_wer': 99.99},\n",
       " 'run_20231213_softplus_all_activation_clean': {'clean_wer': 99.99,\n",
       "  'other_wer': 99.99},\n",
       " 'run_20231211_softplus_clean': {'clean_wer': 8.82, 'other_wer': 23.68},\n",
       " 'run_20231206_doubleswish_clean': {'clean_wer': 8.28, 'other_wer': 22.21}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: the wer results shown here are mix of last epoch (30) and best validation epoch (from step 1.1)\n",
    "# as the author run some experiments individually\n",
    "# Please refer to the paper for the correct wer results, separated by model using best validation epoch and last epoch\n",
    "wer_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis on Computation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all directories with tensorboard files, within each directory, find the latest event file, and parse it to get the epoch durations\n",
    "\n",
    "computing_time = {}\n",
    "\n",
    "# def calculate_avg_epoch_duration(directory='./exp/'):\n",
    "    # get the name of all subdirectories\n",
    "    # for each subdirectory name, prune the name to get the experiment name\n",
    "\n",
    "directory='./exp/'\n",
    "for subdir in os.listdir(directory):\n",
    "    exp_name = subdir[13:]\n",
    "    # print(subdir)\n",
    "    # get the latest event file\n",
    "    event_file = max(glob.glob(directory+ subdir + '/tensorboard/events.out.tfevents.*'), key=os.path.getctime)\n",
    "    # parse the event file to get the epoch durations\n",
    "    epoch_times = {}\n",
    "\n",
    "    for event in tf.compat.v1.train.summary_iterator(event_file):\n",
    "        for value in event.summary.value:\n",
    "            if value.tag == \"train/epoch\":\n",
    "                epoch_number = int(value.simple_value)\n",
    "                if epoch_number not in epoch_times:\n",
    "                    epoch_times[epoch_number] = event.wall_time\n",
    "\n",
    "    \n",
    "    last_available_epoch = min(max(epoch_times.keys()), 30)\n",
    "\n",
    "    if min(epoch_times.keys()) > 25:\n",
    "        first_available_epoch = min(epoch_times.keys())\n",
    "    elif min(epoch_times.keys()) <= 6:\n",
    "        first_available_epoch = min(epoch_times.keys())\n",
    "    else:\n",
    "        first_available_epoch = last_available_epoch - 5\n",
    "\n",
    "\n",
    "    average_duration_raw = (epoch_times[last_available_epoch] - epoch_times[first_available_epoch]) / (last_available_epoch - first_available_epoch) \n",
    "\n",
    "    average_duration_processed = str(datetime.timedelta(seconds=average_duration_raw))[:7]\n",
    "\n",
    "    computing_time[exp_name] = average_duration_processed\n",
    "    \n",
    "    # return computing_time\n",
    "\n",
    "# calculate_avg_epoch_duration(directory='./exp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The average computation time per epoch\n",
    "\n",
    "Calculated using wall time and averaged over the last 5 epoch (since some experiment was not continously run, the wall time was not recorded correctly in the middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gelu_clean': '0:32:16',\n",
       " 'relu_clean': '0:31:19',\n",
       " 'selu_clean': '0:32:11',\n",
       " 'singleswish_clean': '0:33:00',\n",
       " 'tanh_clean': '0:32:02',\n",
       " 'leakyRelu_clean': '0:32:13',\n",
       " 'elu_clean': '0:32:03',\n",
       " 'softplus_all_activation_clean': '0:32:37',\n",
       " 'softplus_clean': '0:32:27',\n",
       " 'doubleswish_clean': '0:33:35'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the average computing time for each experiment\n",
    "computing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaldi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
